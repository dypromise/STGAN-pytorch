class Genc(nn.Module):
    def __init__(self, dim=64, n_layers=5, multi_inputs=1):
        super(Genc, self).__init__()
        in_channels = 3
        self.multi_inputs = multi_inputs
        self.encoder = nn.ModuleList()
        for i in range(n_layers):
            self.encoder.append(nn.Sequential(
                nn.Conv2d(in_channels, dim * 2 ** i, 4, 2, 1, bias=False),
                nn.BatchNorm2d(dim * 2 ** i),
                nn.LeakyReLU(negative_slope=0.2, inplace=True)
            ))
            in_channels = dim * 2 ** i

    def forward(self, x):
        _, c, h, w = x.size()
        zs = []
        x_ = x
        for i, layer in enumerate(self.encoder):
            if self.multi_inputs > i and i > 0:
                x_ = torch.cat([x_, torch.nn.Upsample(
                    size=(h // (2**i), w // (2**i)), mode='bicubic')(x)])
            x_ = layer(x_)
            zs.append(x_)
        return zs


class ConvGRUCell(nn.Module):
    def __init__(self, in_dim, state_dim, out_dim, kernel_size=3):
        super(ConvGRUCell, self).__init__()
        self.upsample = nn.ConvTranspose2d(
            state_dim, out_dim, 4, 2, 1, bias=False)
        self.reset_gate = nn.Sequential(
            nn.Conv2d(in_dim + out_dim, out_dim, kernel_size,
                      1, (kernel_size - 1) // 2, bias=False),
            nn.BatchNorm2d(out_dim),
            nn.Sigmoid()
        )
        self.update_gate = nn.Sequential(
            nn.Conv2d(in_dim + out_dim, out_dim, kernel_size,
                      1, (kernel_size - 1) // 2, bias=False),
            nn.BatchNorm2d(out_dim),
            nn.Sigmoid()
        )
        self.hidden = nn.Sequential(
            nn.Conv2d(in_dim + out_dim, out_dim, kernel_size,
                      1, (kernel_size - 1) // 2, bias=False),
            nn.BatchNorm2d(out_dim),
            nn.Tanh()
        )

    def forward(self, input, old_state):
        n, _, h, w = old_state.size()
        state_ = self.upsample(old_state)
        r = self.reset_gate(torch.cat([input, state_], dim=1))
        z = self.update_gate(torch.cat([input, state_], dim=1))
        new_state = r * state_
        hidden_info = self.hidden(torch.cat([input, new_state], dim=1))
        output = (1 - z) * state_ + z * hidden_info
        return output, new_state


class Gstu(nn.Module):
    def __init__(self, att_dim, dim=64, enc_layers=5, stu_layers=4,
                 inject_layers=0, kernel_size=3, norm='none'):
        super(Gstu, self).__init__()
        self.stu = nn.ModuleList()
        self.stu_layers = stu_layers
        state_dim = att_dim + dim * 2**(enc_layers - 1)
        for i in range(stu_layers):
            in_dim = dim * 2**(stu_layers - 1 - i)
            self.stu.append(ConvGRUCell(in_dim, state_dim,
                                        in_dim, kernel_size=kernel_size))

    def forward(self, zs, _a):
        n, c, h, w = zs[-1].size()
        n_atts = _a.size()[1]
        zs_ = [zs[-1]]
        attr = _a.view((n, n_atts, 1, 1)).expand((n, n_atts, h, w))
        state = torch.cat([zs[-1], attr], dim=1)
        for i in range(self.stu_layers):
            output_, state = self.stu[i](zs[self.stu_layers - 1 - i], state)
            zs_.append(output_)

        return zs_
